---
title: |-
  Definitions
pagenum: 4
prev_page:
  url: /reproducibility/02/whycare.html
next_page:
  url: /reproducibility/04/resources.html
suffix: .md
search: same different reproducible reproducibility research replicable analysis data result definitions computational team detailed robust definition claerbout karrenbach following provide results information turing dataset not used science literature another acm authors measurement experiments using study example provided work generalisable answer replication terms both computer experimental setup obtained stated precision measuring system location multiple trials means independent group obtain artifacts independently scientific findings new disciplines terminology org code software implementation details empirical well statistical described steps performed produces qualitatively similar question pipeline written dependent particular open common noted since popular introduced association computing machinery swapped meaning compared table contrasts heroux et al

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /../content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Definitions</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Definitions-of-reproducibility">Definitions of reproducibility<a class="anchor-link" href="#Definitions-of-reproducibility"> </a></h1><p>The most common definition of reproducibility (and replication) was first noted by Claerbout and Karrenbach in 1992 and has been used in computational science literature since then.
Another popular definition has been introduced in 2013 by the Association for Computing Machinery (ACM), which swapped the meaning of the terms 'reproducible' and 'replicable' compared to Claerbout and Karrenbach.
The following table contrasts both definitions following Heroux et al. (2018).</p>
<table>
<thead><tr>
<th>Term</th>
<th>Claerbout &amp; Karrenbach</th>
<th>ACM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reproducible</td>
<td>Authors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.</td>
<td>(Different team, different experimental setup.) The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently.</td>
</tr>
<tr>
<td>Replicable</td>
<td>A study that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.</td>
<td>(Different team, same experimental setup.) The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author's own artifacts.</td>
</tr>
</tbody>
</table>
<p>Barba (2018) conducted a detailed literature review on the usage of reproducible/replicable covering several disciplines.
Most papers and disciplines use the terminology as defined by Claerbout and Karrenbach, whereas microbiology, immunology and computer science tend to follow the ACM use of reproducibility and replication.
In political science and economics literature, both terms are used interchangeably.</p>
<p>In addition to these high level definitions of reproducibility, some authors provide more detailed disctinctions.
Victoria Stodden <a href="http://edge.org/response-detail/25340">(2014)</a>, a prominent scholar on this topic, has for example identified the following further distinctions:</p>
<ul>
<li><p><em>Computational reproducibility</em>: When detailed information is provided about code, software, hardware and implementation details.</p>
</li>
<li><p><em>Empirical reproducibility</em>: When detailed information is provided about non-computational empirical scientific experiments and observations. In practice this is enabled by making data freely available, as well as details of how the data was collected.</p>
</li>
<li><p><em>Statistical reproducibility</em>: When detailed information is provided, for example, about the choice of statistical tests, model parameters, and threshold values. This mostly relates to pre-registration of study design to prevent p-value hacking and other manipulations.</p>
</li>
</ul>
<h2 id="The-Turing-Way-definition-of-reproducibility">The Turing Way definition of reproducibility<a class="anchor-link" href="#The-Turing-Way-definition-of-reproducibility"> </a></h2><p>At <em>The Turing Way</em> we define <strong>reproducible research</strong> as work that can be independently recreated from the same data and the same code that the original team used.
Reproducible is distinct from replicable, robust and generalisable as described in the figure below.</p>
<table>
<thead><tr>
<th><img src="../../figures/reproducibility/ReproducibleMatrix.jpg" alt="Kirstie&#39;s definition of reproducible research"></th>
</tr>
</thead>
<tbody>
<tr>
<td>How the Turing Way defines reproducible research</td>
</tr>
</tbody>
</table>
<p>The different dimensions of reproducible research described in the matrix above have the following definitions:</p>
<ul>
<li><strong>Reproducible:</strong> A result is reproducible when the <em>same</em> analysis steps performed on the <em>same</em> dataset consistently produces the <em>same</em> answer.</li>
<li><strong>Replicable:</strong> A result is replicable when the <em>same</em> analysis performed on <em>different</em> datasets produces qualitatively similar answers.</li>
<li><strong>Robust:</strong> A result is robust when the <em>same</em> dataset is subjected to <em>different</em> analysis workflows to answer the same research question (for example one pipeline written in R and another written in Python) and a qualitatively similar or identical answer is produced.
Robust results show that the work is not dependent on the specificities of the programming language chosen to perform the analysis.</li>
<li><strong>Generalisable:</strong> Combining replicable and robust findings allow us to form generalisable results.
Note that running an analysis on a different software implementation and with a different dataset does not provide <em>generalised</em> results.
There will be many more steps to know how well the work applies to all the different aspects of the research question.
Generalisation is an important step towards understanding that the result is not dependent on a particular dataset nor a particular version of the analysis pipeline.</li>
</ul>
<p>More information on these definitions can be found in <a href="https://www.frontiersin.org/articles/10.3389/fninf.2017.00076/full">"Reproducibility vs. Replicability: A Brief History of a Confused Terminology" by Hans E. Plesser</a>.</p>
<h2 id="Reproducible-but-not-open">Reproducible but not open<a class="anchor-link" href="#Reproducible-but-not-open"> </a></h2><p><em>The Turing Way</em> recognises that some research will use sensitive data that cannot be shared and this handbook will provide guides on how your research can be reproducible without all parts necessarily being open.</p>

</div>
</div>
</div>
</div>

 


    </main>
    